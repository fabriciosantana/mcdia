{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama 3.2 3B Instruct - Hugging Face API\n",
    "\n",
    "Este notebook demonstra como usar o modelo **meta-llama/Llama-3.2-3B-Instruct** via Hugging Face Inference API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Instalar depend√™ncias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers huggingface_hub accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configurar autentica√ß√£o\n",
    "\n",
    "Voc√™ precisa de um token do Hugging Face. Obtenha em: https://huggingface.co/settings/tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Autenticado no Hugging Face!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Op√ß√£o 1: Definir como vari√°vel de ambiente\n",
    "#os.environ[\"HF_TOKEN\"] =\n",
    "\n",
    "# Op√ß√£o 2: Login interativo (vai pedir o token)\n",
    "# login()\n",
    "\n",
    "# Op√ß√£o 3: Passar o token diretamente (n√£o recomendado para c√≥digo p√∫blico)\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\", None)\n",
    "\n",
    "if HF_TOKEN:\n",
    "    login(token=HF_TOKEN)\n",
    "    print(\"‚úÖ Autenticado no Hugging Face!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Defina HF_TOKEN ou rode login() para se autenticar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Usar via Inference API (modo mais simples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Learning √© uma sub√°rea da Intelig√™ncia Artificial que utiliza redes neurais artificiais para processar e aprender padr√µes em grandes conjuntos de dados. Essa abordagem permite que os sistemas aprendam a reconhecer padr√µes e fazer previs√µes ou classifica√ß√µes com alta precis√£o, sem a necessidade de programa√ß√£o expl√≠cita. A Deep Learning √© amplamente utilizada em diversas √°reas, como vis√£o computacional, processamento de linguagem natural e reconhecimento de voz.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(token=HF_TOKEN)\n",
    "\n",
    "# Chat b√°sico\n",
    "response = client.chat.completions.create(\n",
    "    model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Explique o que √© Deep Learning em 3 frases.\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Chat com contexto (m√∫ltiplas mensagens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN (Redes Neurais Recorrentes) e Transformer s√£o dois tipos de redes neurais usadas em processamento de linguagem natural (NLP) e processamento de texto. Aqui est√£o as principais diferen√ßas entre eles:\n",
      "\n",
      "**RNN (Redes Neurais Recorrentes)**\n",
      "\n",
      "*   S√£o redes neurais que processam sequ√™ncias de dados de forma recorrente, ou seja, elas consideram a sequ√™ncia de dados como um todo ao fazer previs√µes.\n",
      "*   Usam um par√¢metro chamado \"estado oculto\" para armazenar informa√ß√µes sobre a sequ√™ncia de dados.\n",
      "*   S√£o mais dif√≠ceis de treinar do que as redes neurais convencionais, pois precisam lidar com a depend√™ncia temporal.\n",
      "*   S√£o mais adequadas para tarefas que envolvem sequ√™ncias de dados, como previs√£o de sequ√™ncias, reconhecimento de padr√µes e processamento de linguagem natural.\n",
      "\n",
      "**Transformer**\n",
      "\n",
      "*   S√£o redes neurais que processam sequ√™ncias de dados de forma paralela, ou seja, elas processam cada elemento da sequ√™ncia de forma independente.\n",
      "*   N√£o usam um estado oculto, em vez disso, usam aten√ß√£o para capturar rela√ß√µes entre diferentes elementos da sequ√™ncia.\n",
      "*   S√£o mais f√°ceis de treinar do que as redes neurais recorrentes, pois n√£o precisam lidar com a depend√™ncia temporal.\n",
      "*   S√£o mais adequadas para tarefas que envolvem processamento de sequ√™ncias de dados, como tradu√ß√£o autom√°tica, processamento de texto e reconhecimento de voz.\n",
      "\n",
      "Em resumo, as redes neurais recorrentes s√£o mais adequadas para tarefas que envolvem sequ√™ncias de dados e depend√™ncia temporal, enquanto as redes neurais transformadoras s√£o mais adequadas para tarefas que envolvem processamento de sequ√™ncias de dados de forma paralela e sem depend√™ncia temporal.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Voc√™ √© um assistente especializado em IA. Seja conciso e t√©cnico.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Qual a diferen√ßa entre RNN e Transformer?\"},\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Streaming (resposta em tempo real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Llama 3.2: "
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mü§ñ Llama 3.2: \u001b[39m\u001b[33m\"\u001b[39m, end=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[43mclient\u001b[49m.chat.completions.create(\n\u001b[32m      3\u001b[39m     model=\u001b[33m\"\u001b[39m\u001b[33mmeta-llama/Llama-3.2-3B-Instruct\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      4\u001b[39m     messages=[{\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mListe 3 aplica√ß√µes pr√°ticas de LLMs.\u001b[39m\u001b[33m\"\u001b[39m}],\n\u001b[32m      5\u001b[39m     stream=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m      6\u001b[39m ):\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m chunk.choices[\u001b[32m0\u001b[39m].delta.content:\n\u001b[32m      8\u001b[39m         \u001b[38;5;28mprint\u001b[39m(chunk.choices[\u001b[32m0\u001b[39m].delta.content, end=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m, flush=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"ü§ñ Llama 3.2: \", end=\"\")\n",
    "for chunk in client.chat.completions.create(\n",
    "    model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Liste 3 aplica√ß√µes pr√°ticas de LLMs.\"}],\n",
    "    stream=True\n",
    "):\n",
    "    if chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Com par√¢metros avan√ßados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aqui est√£o algumas sugest√µes de t√≠tulos criativos para um artigo sobre IA:\n",
      "\n",
      "1. \"A Intelig√™ncia Artificial: O Futuro que Estamos Construindo\"\n",
      "2. \"O Tempo de IA: Como a Intel\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Gerar um t√≠tulo criativo para um artigo sobre IA.\"}],\n",
    "    temperature=0.9,\n",
    "    max_tokens=50,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Usar via API REST direta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'choices'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m response = requests.post(API_URL, headers=headers, json=payload)\n\u001b[32m     12\u001b[39m result = response.json()\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mchoices\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[31mKeyError\u001b[39m: 'choices'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/meta-llama/Llama-3.2-3B-Instruct/v1/chat/completions\"\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {HF_TOKEN}\"}\n",
    "payload = {\n",
    "    \"model\": \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Diga ol√° em portugu√™s.\"}],\n",
    "}\n",
    "\n",
    "response = requests.post(API_URL, headers=headers, json=payload)\n",
    "result = response.json()\n",
    "print(result[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Carregar modelo localmente (requer GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Carregar tokenizer e modelo\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=HF_TOKEN)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    token=HF_TOKEN,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Modelo carregado: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Gera√ß√£o local com transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatar prompt no estilo Llama\n",
    "def format_prompt(user_input, system_prompt=\"\"):\n",
    "    if system_prompt:\n",
    "        return f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n{user_input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
    "    return f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n{user_input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
    "\n",
    "prompt = format_prompt(\"Explique transformers em uma frase.\", \"Seja t√©cnico e conciso.\")\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.7,\n",
    "    do_sample=True,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Pipeline simplificado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Criar pipeline de text generation\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    token=HF_TOKEN,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"Qual a capital do Brasil?\"}]\n",
    "outputs = pipe(messages, max_new_tokens=50)\n",
    "print(outputs[0][\"generated_text\"][-1][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notas\n",
    "\n",
    "- **Inference API**: Mais simples, n√£o requer GPU, mas tem limites de rate\n",
    "- **Modelo local**: Mais controle, sem limites, mas requer GPU (pelo menos 8GB VRAM para 3B)\n",
    "- **Token**: Necess√°rio aceitar os termos do modelo em https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcdia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
