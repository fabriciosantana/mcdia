{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama 3.2 3B Instruct - Hugging Face API\n",
    "\n",
    "Este notebook demonstra como usar o modelo **meta-llama/Llama-3.2-3B-Instruct** via Hugging Face Inference API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Instalar depend√™ncias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers huggingface_hub accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configurar autentica√ß√£o\n",
    "\n",
    "Voc√™ precisa de um token do Hugging Face. Obtenha em: https://huggingface.co/settings/tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "Invalid user token. The token from HF_TOKEN environment variable is invalid. Note that HF_TOKEN takes precedence over `hf auth login`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/mcdia/lib/python3.13/site-packages/huggingface_hub/utils/_http.py:402\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/mcdia/lib/python3.13/site-packages/requests/models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 401 Client Error: Unauthorized for url: https://huggingface.co/api/whoami-v2",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mHfHubHTTPError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/mcdia/lib/python3.13/site-packages/huggingface_hub/hf_api.py:1800\u001b[39m, in \u001b[36mHfApi.whoami\u001b[39m\u001b[34m(self, token)\u001b[39m\n\u001b[32m   1799\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1801\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/mcdia/lib/python3.13/site-packages/huggingface_hub/utils/_http.py:475\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    473\u001b[39m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[32m    474\u001b[39m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mHfHubHTTPError\u001b[39m: 401 Client Error: Unauthorized for url: https://huggingface.co/api/whoami-v2 (Request ID: Root=1-69a3024a-33bb5b642fa5697c34d870b5;836e6583-0a60-4b6d-8714-c7df7b26f6a2)\n\nInvalid username or password.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[32m/tmp/ipykernel_24102/2147596662.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Op√ß√£o 3: Passar o token diretamente (n√£o recomendado para c√≥digo p√∫blico)\u001b[39;00m\n\u001b[32m     11\u001b[39m HF_TOKEN = os.environ.get(\u001b[33m\"HF_TOKEN\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     12\u001b[39m \n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m HF_TOKEN:\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     login(token=HF_TOKEN)\n\u001b[32m     15\u001b[39m     print(\u001b[33m\"‚úÖ Autenticado no Hugging Face!\"\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     17\u001b[39m     print(\u001b[33m\"‚ö†Ô∏è Defina HF_TOKEN ou rode login() para se autenticar\"\u001b[39m)\n",
      "\u001b[32m/opt/conda/envs/mcdia/lib/python3.13/site-packages/huggingface_hub/utils/_deprecation.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     97\u001b[39m                 )\n\u001b[32m     98\u001b[39m                 \u001b[38;5;28;01mif\u001b[39;00m custom_message \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     99\u001b[39m                     message += \u001b[33m\"\\n\\n\"\u001b[39m + custom_message\n\u001b[32m    100\u001b[39m                 warnings.warn(message, FutureWarning)\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m f(*args, **kwargs)\n",
      "\u001b[32m/opt/conda/envs/mcdia/lib/python3.13/site-packages/huggingface_hub/utils/_deprecation.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     27\u001b[39m         @wraps(f)\n\u001b[32m     28\u001b[39m         \u001b[38;5;28;01mdef\u001b[39;00m inner_f(*args, **kwargs):\n\u001b[32m     29\u001b[39m             extra_args = len(args) - len(all_args)\n\u001b[32m     30\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m extra_args <= \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m f(*args, **kwargs)\n\u001b[32m     32\u001b[39m             \u001b[38;5;66;03m# extra_args > 0\u001b[39;00m\n\u001b[32m     33\u001b[39m             args_msg = [\n\u001b[32m     34\u001b[39m                 f\"{name}='{arg}'\" \u001b[38;5;28;01mif\u001b[39;00m isinstance(arg, str) \u001b[38;5;28;01melse\u001b[39;00m f\"{name}={arg}\"\n",
      "\u001b[32m/opt/conda/envs/mcdia/lib/python3.13/site-packages/huggingface_hub/_login.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(token, add_to_git_credential, new_session, write_permission)\u001b[39m\n\u001b[32m    116\u001b[39m                 \u001b[33m\"`add_to_git_credential=True` in this function directly or \"\u001b[39m\n\u001b[32m    117\u001b[39m                 \u001b[33m\"`--add-to-git-credential` if using via `hf`CLI if \"\u001b[39m\n\u001b[32m    118\u001b[39m                 \u001b[33m\"you want to set the git credential as well.\"\u001b[39m\n\u001b[32m    119\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         _login(token, add_to_git_credential=add_to_git_credential)\n\u001b[32m    121\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m is_notebook():\n\u001b[32m    122\u001b[39m         notebook_login(new_session=new_session)\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m/opt/conda/envs/mcdia/lib/python3.13/site-packages/huggingface_hub/_login.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(token, add_to_git_credential)\u001b[39m\n\u001b[32m    394\u001b[39m \n\u001b[32m    395\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m token.startswith(\u001b[33m\"api_org\"\u001b[39m):\n\u001b[32m    396\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ValueError(\u001b[33m\"You must use your personal account token, not an organization token.\"\u001b[39m)\n\u001b[32m    397\u001b[39m \n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m     token_info = whoami(token)\n\u001b[32m    399\u001b[39m     permission = token_info[\u001b[33m\"auth\"\u001b[39m][\u001b[33m\"accessToken\"\u001b[39m][\u001b[33m\"role\"\u001b[39m]\n\u001b[32m    400\u001b[39m     logger.info(f\"Token is valid (permission: {permission}).\")\n\u001b[32m    401\u001b[39m \n",
      "\u001b[32m/opt/conda/envs/mcdia/lib/python3.13/site-packages/huggingface_hub/utils/_validators.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    110\u001b[39m \n\u001b[32m    111\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m             kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.__name__, has_token=has_token, kwargs=kwargs)\n\u001b[32m    113\u001b[39m \n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m fn(*args, **kwargs)\n",
      "\u001b[32m/opt/conda/envs/mcdia/lib/python3.13/site-packages/huggingface_hub/hf_api.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, token)\u001b[39m\n\u001b[32m   1810\u001b[39m                         \u001b[33m\"Note that HF_TOKEN takes precedence over `hf auth login`.\"\u001b[39m\n\u001b[32m   1811\u001b[39m                     )\n\u001b[32m   1812\u001b[39m                 \u001b[38;5;28;01melif\u001b[39;00m effective_token == _get_token_from_file():\n\u001b[32m   1813\u001b[39m                     error_message += \u001b[33m\" The token stored is invalid. Please run `hf auth login` to update it.\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1814\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(error_message, request=e.request, response=e.response) \u001b[38;5;28;01mfrom\u001b[39;00m e\n\u001b[32m   1815\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m   1816\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m r.json()\n",
      "\u001b[31mHTTPError\u001b[39m: Invalid user token. The token from HF_TOKEN environment variable is invalid. Note that HF_TOKEN takes precedence over `hf auth login`."
     ]
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Op√ß√£o 1: Definir como vari√°vel de ambiente\n",
    "os.environ[\"HF_TOKEN\"] = \"HF_TOKEN_AQUI\" \n",
    "\n",
    "# Op√ß√£o 2: Login interativo (vai pedir o token)\n",
    "# login()\n",
    "\n",
    "# Op√ß√£o 3: Passar o token diretamente (n√£o recomendado para c√≥digo p√∫blico)\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\", None)\n",
    "\n",
    "if HF_TOKEN:\n",
    "    login(token=HF_TOKEN)\n",
    "    print(\"‚úÖ Autenticado no Hugging Face!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Defina HF_TOKEN ou rode login() para se autenticar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Usar via Inference API (modo mais simples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(token=HF_TOKEN)\n",
    "\n",
    "# Chat b√°sico\n",
    "response = client.chat.completions.create(\n",
    "    model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Explique o que √© Deep Learning em 3 frases.\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Chat com contexto (m√∫ltiplas mensagens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Voc√™ √© um assistente especializado em IA. Seja conciso e t√©cnico.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Qual a diferen√ßa entre RNN e Transformer?\"},\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Streaming (resposta em tempo real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ü§ñ Llama 3.2: \", end=\"\")\n",
    "for chunk in client.chat.completions.create(\n",
    "    model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Liste 3 aplica√ß√µes pr√°ticas de LLMs.\"}],\n",
    "    stream=True\n",
    "):\n",
    "    if chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Com par√¢metros avan√ßados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Gerar um t√≠tulo criativo para um artigo sobre IA.\"}],\n",
    "    temperature=0.9,\n",
    "    max_tokens=50,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Usar via API REST direta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/meta-llama/Llama-3.2-3B-Instruct/v1/chat/completions\"\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {HF_TOKEN}\"}\n",
    "payload = {\n",
    "    \"model\": \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Diga ol√° em portugu√™s.\"}],\n",
    "}\n",
    "\n",
    "response = requests.post(API_URL, headers=headers, json=payload)\n",
    "result = response.json()\n",
    "print(result[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Carregar modelo localmente (requer GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Carregar tokenizer e modelo\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=HF_TOKEN)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    token=HF_TOKEN,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Modelo carregado: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Gera√ß√£o local com transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatar prompt no estilo Llama\n",
    "def format_prompt(user_input, system_prompt=\"\"):\n",
    "    if system_prompt:\n",
    "        return f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n{user_input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
    "    return f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n{user_input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
    "\n",
    "prompt = format_prompt(\"Explique transformers em uma frase.\", \"Seja t√©cnico e conciso.\")\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.7,\n",
    "    do_sample=True,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Pipeline simplificado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Criar pipeline de text generation\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    token=HF_TOKEN,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"Qual a capital do Brasil?\"}]\n",
    "outputs = pipe(messages, max_new_tokens=50)\n",
    "print(outputs[0][\"generated_text\"][-1][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notas\n",
    "\n",
    "- **Inference API**: Mais simples, n√£o requer GPU, mas tem limites de rate\n",
    "- **Modelo local**: Mais controle, sem limites, mas requer GPU (pelo menos 8GB VRAM para 3B)\n",
    "- **Token**: Necess√°rio aceitar os termos do modelo em https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcdia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
