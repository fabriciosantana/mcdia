{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NVIDIA Nemotron - Exemplos de Uso\n",
    "\n",
    "O **Nemotron** √© uma fam√≠lia de modelos da NVIDIA com arquitetura h√≠brida Mamba-Transformer MoE.\n",
    "\n",
    "Este notebook mostra como usar o Nemotron via NVIDIA NIM API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configurar API Key\n",
    "\n",
    "A chave est√° em `~/.bashrc`. Esta c√©lula carrega automaticamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ NVIDIA_API_KEY carregada: nvapi-8tdO1NjXD...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def get_nvidia_api_key():\n",
    "    # Primeiro, verificar se j√° est√° no ambiente\n",
    "    key = os.environ.get(\"NVIDIA_API_KEY\")\n",
    "    if key:\n",
    "        return key\n",
    "    \n",
    "    # Tentar carregar do bashrc\n",
    "    bashrc_path = os.path.expanduser(\"~/.bashrc\")\n",
    "    try:\n",
    "        with open(bashrc_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                if line.startswith(\"export NVIDIA_API_KEY=\"):\n",
    "                    key = line.split(\"=\", 1)[1].strip().strip('\"').strip(\"'\")\n",
    "                    os.environ[\"NVIDIA_API_KEY\"] = key\n",
    "                    return key\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao ler bashrc: {e}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "NVIDIA_API_KEY = get_nvidia_api_key()\n",
    "\n",
    "if NVIDIA_API_KEY:\n",
    "    print(f\"‚úÖ NVIDIA_API_KEY carregada: {NVIDIA_API_KEY[:15]}...\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è NVIDIA_API_KEY n√£o encontrada. Defina em ~/.bashrc ou manualmente:\")\n",
    "    print('os.environ[\"NVIDIA_API_KEY\"] = \"sua_chave_aqui\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Testar conex√£o com NVIDIA API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Resposta do Nemotron:\n",
      "\n",
      "Deep Learning √© um ramo do aprendizado de m√°quina que utiliza redes neurais artificiais com m√∫ltiplas camadas ocultas para aprender representa√ß√µes hier√°rquicas de dados. Essas redes s√£o capazes de extrair caracter√≠sticas cada vez mais abstratas, desde pixels brutos at√© objetosComplexos, sem necessidade de engenharia manual das caracter√≠sticas. Assim, o Deep Learning tem possibilitado avan√ßos significativos em √°reas como vis√£o computacional, processamento de linguagem natural e reconhecimento de voz.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "invoke_url = \"https://integrate.api.nvidia.com/v1/chat/completions\"\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {NVIDIA_API_KEY}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "payload = {\n",
    "    \"model\": \"nvidia/nemotron-3-nano-30b-a3b\",\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Explique o que √© Deep Learning em 3 frases.\"}\n",
    "    ],\n",
    "    \"temperature\": 0.7,\n",
    "    \"max_tokens\": 200\n",
    "}\n",
    "\n",
    "response = requests.post(invoke_url, headers=headers, json=payload)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(\"‚úÖ Resposta do Nemotron:\")\n",
    "    print(result[\"choices\"][0][\"message\"][\"content\"])\n",
    "else:\n",
    "    print(f\"‚ùå Erro {response.status_code}: {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Usando a biblioteca OpenAI (compat√≠vel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Diferen√ßas principais entre **RNN** (Redes Neurais Recorrentes) e **Transformer**\n",
      "\n",
      "| Aspecto | RNN (Recorrente) | Transformer |\n",
      "|---|---|---|\n",
      "| **Arquitetura b√°sica** | Processa a sequ√™ncia **um token de cada vez**, mantendo um estado interno (hidden state) que √© passado de um passo para o pr√≥ximo. | Baseado exclusivamente em **mecanismo de aten√ß√£o** (self‚Äëattention) e em camadas feed‚Äëforward; n√£o h√° conex√£o recorrente expl√≠cita entre tokens. |\n",
      "| **Depend√™ncia de ordem** | A ordem dos tokens √© inerente ao algoritmo: o pr√≥ximo passo s√≥ pode ser calculado depois que o anterior foi processado. | A ordem pode ser informada por **posicionais encodings** adicionados √†s representa√ß√µes; a aten√ß√£o pode combinar informa√ß√µes de qualquer posi√ß√£o simultaneamente. |\n",
      "| **Paral\n"
     ]
    }
   ],
   "source": [
    "# pip install openai\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://integrate.api.nvidia.com/v1\",\n",
    "    api_key=NVIDIA_API_KEY\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"nvidia/nemotron-3-nano-30b-a3b\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Qual a diferen√ßa entre RNN e Transformer?\"}\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=300\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Streaming (resposta em tempo real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Nemotron: \n",
      "**3 aplica√ß√µes pr√°ticas de LLMs (Large Language Models)**  \n",
      "\n",
      "1. **Assistentes virtuais e chatbots corporativos**  \n",
      "   - Automatizam o atendimento ao cliente, respondendo a d√∫vidas frequentes, realizando suporte t√©cnico e orientando usu√°rios em processos de compra ou suporte, tudo com linguagem natural e contextualizada.\n",
      "\n",
      "2. **Gera√ß√£o e resumo de conte√∫do**  \n",
      "   - Produzem textos como artigos, relat√≥rios, legendas para redes sociais, newsletters e roteiros, al√©m de resumir documentos extensos, permitindo que equipes criem material de alta qualidade em menos tempo.\n",
      "\n",
      "3. **An√°lise e extra√ß√£o de informa√ß√µes de grandes volumes de texto"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mü§ñ Nemotron: \u001b[39m\u001b[33m\"\u001b[39m, end=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m client.chat.completions.create(\n\u001b[32m      4\u001b[39m     model=\u001b[33m\"\u001b[39m\u001b[33mnvidia/nemotron-3-nano-30b-a3b\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m     messages=[{\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mListe 3 aplica√ß√µes pr√°ticas de LLMs.\u001b[39m\u001b[33m\"\u001b[39m}],\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m     stream=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m      9\u001b[39m ):\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mchunk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchoices\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m.delta.content:\n\u001b[32m     11\u001b[39m         \u001b[38;5;28mprint\u001b[39m(chunk.choices[\u001b[32m0\u001b[39m].delta.content, end=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m, flush=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m()\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "print(\"ü§ñ Nemotron: \", end=\"\")\n",
    "\n",
    "for chunk in client.chat.completions.create(\n",
    "    model=\"nvidia/nemotron-3-nano-30b-a3b\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Liste 3 aplica√ß√µes pr√°ticas de LLMs.\"}],\n",
    "    temperature=0.7,\n",
    "    max_tokens=200,\n",
    "    stream=True\n",
    "):\n",
    "    if chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Chat com contexto (m√∫ltiplas mensagens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Aten√ß√£o em Transformers**  \n",
      "\n",
      "A aten√ß√£o (attention) √© a opera√ß√£o central que permite que um Transformer ‚Äúolhe‚Äù para todas as posi√ß√µes de uma sequ√™ncia ao processar cada token. Ela calcula **peso de import√¢ncia** de cada token de entrada em rela√ß√£o a todos os demais (ou a um conjunto selecionado) antes de gerar a sa√≠da.\n",
      "\n",
      "---\n",
      "\n",
      "## 1. Aten√ß√£o Scaled‚ÄëDot‚ÄëProduct (SDPA)\n",
      "\n",
      "Para um conjunto de *queries* **Q**, *keys* **K** e *values* **V** (matrizes de dimens√£o *d_k √ó n*), a aten√ß√£o √© definida como:\n",
      "\n",
      "\\[\n",
      "\\text{Attention}(Q, K, V) \\;=\\; \\text{softmax}\\!\\left(\\frac{QK^{\\top}}{\\sqrt{d_k}}\\right) V\n",
      "\\]\n",
      "\n",
      "- **QK·µÄ** ‚Üí produto ponto entre consultas e chaves, gerando uma matriz de compatibilidade de tamanho *n √ó n*.\n",
      "- **\\sqrt{d_k}** ‚Üí escala para evitar gradientes explosivos.\n",
      "- **softmax** ‚Üí converte as compatibilidades em probabilidades (peso de aten√ß√£o) na faixa (0,1).\n",
      "- **\\* V** ‚Üí combina os *values* ponderados pelos pesos de aten√ß√£o.\n",
      "\n",
      "---\n",
      "\n",
      "## 2. Tipos de aten√ß√£o\n",
      "\n",
      "| Tipo | Quando usado | Caracter√≠sticas |\n",
      "|------|--------------|-----------------|\n",
      "| **Self‚Äëattention** | Dentro de uma mesma sequ√™ncia (ex.: encoder ou decoder). | Cada token pode ‚Äúatender‚Äù a todos os outros tokens da mesma sequ√™ncia. |\n",
      "| **Cross‚Äëattention** | Entre duas sequ√™ncias diferentes (ex.: decoder que atende ao encoder). | O *query* vem da sequ√™ncia alvo; *key* e *value* v√™m da outra sequ√™ncia. |\n",
      "| **Multi‚Äëhead attention** | V√°rias ‚Äúcabe√ßas‚Äù paralelas (h) que projetam diferentes subespa√ßos de Q, K, V. | Captura rela√ß√µes de diferentes tipos simultaneamente; depois concatena os resultados. |\n",
      "| **Sparse / Local attention** | Estruturas de aten√ß√£o limitada (ex.: apenas vizinhos ou janelas). | Reduz complexidade O(n¬≤) ‚Üí O(n¬∑w) (w = janela). |\n",
      "\n",
      "---\n",
      "\n",
      "## 3. Por que a aten√ß√£o funciona?\n",
      "\n",
      "1. **Depend√™ncia de longo alcance**: Cada token pode interagir diretamente com qualquer outro, independentemente da dist√¢ncia posicional.\n",
      "2. **Flexibilidade de modelo**: Os pesos s√£o aprendidos de forma end‚Äëto‚Äëend, permitindo que a rede descubra padr√µes de depend√™ncia sem regras pr√©‚Äëdefinidas.\n",
      "3. **Paralelismo**: O c√°lculo de Q, K, V e da matriz de aten√ß√£o √© totalmente vetorizado, permitindo treinamento r√°pido em GPUs/TPUs.\n",
      "\n",
      "---\n",
      "\n",
      "## 4. Complexidade e otimiza√ß√µes\n",
      "\n",
      "- **Complexidade cl√°ssica**: O(n¬≤¬∑d_k) (n = comprimento da sequ√™ncia).  \n",
      "- **Solu√ß√µes**:  \n",
      "  - *Linear attention* (e.g., Performer, Linformer) ‚Üí O(n¬∑d_k).  \n",
      "  - *Sparse attention* (e.g., Longformer, BigBird) ‚Üí O(n¬∑w).  \n",
      "  - *Chunked/recurrent* attention para sequ√™ncias muito longas.\n",
      "\n",
      "---\n",
      "\n",
      "## 5. Resumo conciso\n",
      "\n",
      "- **Aten√ß√£o** = c√°lculo de pesos de similaridade entre queries e keys ‚Üí distribui√ß√£o por softmax ‚Üí combina√ß√£o com values.  \n",
      "- Permite que cada posi√ß√£o da sequ√™ncia **agregue informa√ß√µes** de todas as outras posi√ß√µes de forma adaptativa.  \n",
      "- Implementada como **scaled dot‚Äëproduct attention**, geralmente multiplicada por m√∫ltiplas *heads* (multi‚Äëhead) para capturar diferentes tipos de rela√ß√£o.  \n",
      "- √â o mecanismo que substitui recursividade ou convolu√ß√£o nas redes Transformer, habilitando modelos como BERT, GPT, T5, etc.\n",
      "\n",
      "Em poucas palavras: a aten√ß√£o √© um **peso aprendido que mede qu√£o relevante cada token √© para a compreens√£o de outro token**, permitindo que o modelo capture depend√™ncias complexas e longas de forma eficiente e paraleliz√°vel.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Voc√™ √© um especialista em IA. Seja t√©cnico e conciso.\"},\n",
    "    {\"role\": \"user\", \"content\": \"O que √© aten√ß√£o em transformers?\"},\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"nvidia/nemotron-3-nano-30b-a3b\",\n",
    "    messages=messages,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Par√¢metros avan√ßados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"nvidia/nemotron-3-nano-30b-a3b\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Gere um t√≠tulo criativo para um artigo sobre IA.\"}],\n",
    "    temperature=0.9,\n",
    "    max_tokens=50,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos Nemotron Dispon√≠veis\n",
    "\n",
    "| Modelo | Tamanho | Descri√ß√£o |\n",
    "|--------|---------|----------|\n",
    "| `nvidia/nemotron-3-nano-30b-a3b` | 30B (3.5B ativos) | MoE h√≠brido, reasoning |\n",
    "| `nvidia/nemotron-4-340b-instruct` | 340B | Modelo grande para tarefas complexas |\n",
    "\n",
    "### Caracter√≠sticas:\n",
    "- **Arquitetura**: Mamba2-Transformer Hybrid MoE\n",
    "- **Idiomas**: Ingl√™s, Alem√£o, Espanhol, Franc√™s, Italiano, Japon√™s\n",
    "- **Capacidades**: Reasoning, tool calling, chat, c√≥digo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcdia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
