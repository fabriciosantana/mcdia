{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NVIDIA Nemotron - Exemplos de Uso\n",
    "\n",
    "O **Nemotron** √© uma fam√≠lia de modelos da NVIDIA com arquitetura h√≠brida Mamba-Transformer MoE.\n",
    "\n",
    "Este notebook mostra como usar o Nemotron via NVIDIA NIM API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configurar API Key\n",
    "\n",
    "A chave est√° em `~/.bashrc`. Esta c√©lula carrega automaticamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_nvidia_api_key():\n",
    "    # Primeiro, verificar se j√° est√° no ambiente\n",
    "    key = os.environ.get(\"NVIDIA_API_KEY\")\n",
    "    if key:\n",
    "        return key\n",
    "    \n",
    "    # Tentar carregar do bashrc\n",
    "    bashrc_path = os.path.expanduser(\"~/.bashrc\")\n",
    "    try:\n",
    "        with open(bashrc_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                if line.startswith(\"export NVIDIA_API_KEY=\"):\n",
    "                    key = line.split(\"=\", 1)[1].strip().strip('\"').strip(\"'\")\n",
    "                    os.environ[\"NVIDIA_API_KEY\"] = key\n",
    "                    return key\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao ler bashrc: {e}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "NVIDIA_API_KEY = get_nvidia_api_key()\n",
    "\n",
    "if NVIDIA_API_KEY:\n",
    "    print(f\"‚úÖ NVIDIA_API_KEY carregada: {NVIDIA_API_KEY[:15]}...\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è NVIDIA_API_KEY n√£o encontrada. Defina em ~/.bashrc ou manualmente:\")\n",
    "    print('os.environ[\"NVIDIA_API_KEY\"] = \"sua_chave_aqui\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Testar conex√£o com NVIDIA API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "invoke_url = \"https://integrate.api.nvidia.com/v1/chat/completions\"\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {NVIDIA_API_KEY}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "payload = {\n",
    "    \"model\": \"nvidia/nemotron-3-nano-30b-a3b\",\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Explique o que √© Deep Learning em 3 frases.\"}\n",
    "    ],\n",
    "    \"temperature\": 0.7,\n",
    "    \"max_tokens\": 200\n",
    "}\n",
    "\n",
    "response = requests.post(invoke_url, headers=headers, json=payload)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(\"‚úÖ Resposta do Nemotron:\")\n",
    "    print(result[\"choices\"][0][\"message\"][\"content\"])\n",
    "else:\n",
    "    print(f\"‚ùå Erro {response.status_code}: {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Usando a biblioteca OpenAI (compat√≠vel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install openai\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://integrate.api.nvidia.com/v1\",\n",
    "    api_key=NVIDIA_API_KEY\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"nvidia/nemotron-3-nano-30b-a3b\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Qual a diferen√ßa entre RNN e Transformer?\"}\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=300\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Streaming (resposta em tempo real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ü§ñ Nemotron: \", end=\"\")\n",
    "\n",
    "for chunk in client.chat.completions.create(\n",
    "    model=\"nvidia/nemotron-3-nano-30b-a3b\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Liste 3 aplica√ß√µes pr√°ticas de LLMs.\"}],\n",
    "    temperature=0.7,\n",
    "    max_tokens=200,\n",
    "    stream=True\n",
    "):\n",
    "    if chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Chat com contexto (m√∫ltiplas mensagens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Voc√™ √© um especialista em IA. Seja t√©cnico e conciso.\"},\n",
    "    {\"role\": \"user\", \"content\": \"O que √© aten√ß√£o em transformers?\"},\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"nvidia/nemotron-3-nano-30b-a3b\",\n",
    "    messages=messages,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Par√¢metros avan√ßados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"nvidia/nemotron-3-nano-30b-a3b\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Gere um t√≠tulo criativo para um artigo sobre IA.\"}],\n",
    "    temperature=0.9,\n",
    "    max_tokens=50,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos Nemotron Dispon√≠veis\n",
    "\n",
    "| Modelo | Tamanho | Descri√ß√£o |\n",
    "|--------|---------|----------|\n",
    "| `nvidia/nemotron-3-nano-30b-a3b` | 30B (3.5B ativos) | MoE h√≠brido, reasoning |\n",
    "| `nvidia/nemotron-4-340b-instruct` | 340B | Modelo grande para tarefas complexas |\n",
    "\n",
    "### Caracter√≠sticas:\n",
    "- **Arquitetura**: Mamba2-Transformer Hybrid MoE\n",
    "- **Idiomas**: Ingl√™s, Alem√£o, Espanhol, Franc√™s, Italiano, Japon√™s\n",
    "- **Capacidades**: Reasoning, tool calling, chat, c√≥digo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
