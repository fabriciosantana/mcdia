{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama 3.2 3B Instruct - Exemplo com Ollama\n",
    "\n",
    "Este notebook demonstra como usar o modelo **Llama 3.2 3B Instruct** localmente via Ollama."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Verificar se o Ollama est√° rodando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ollama rodando: Ollama is running\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "try:\n",
    "    response = requests.get(\"http://localhost:11434\")\n",
    "    print(f\"‚úÖ Ollama rodando: {response.text.strip()}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Ollama n√£o est√° rodando. Erro: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Listar modelos dispon√≠veis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME           ID              SIZE      MODIFIED       \n",
      "llama3.2:3b    a80c4f17acd5    2.0 GB    15 minutes ago    \n",
      "glm-5:cloud    c313cd065935    -         51 minutes ago    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "result = subprocess.run([\"ollama\", \"list\"], capture_output=True, text=True)\n",
    "print(result.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Usar o modelo com a biblioteca `ollama`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Learning √© uma sub√°rea da Intelig√™ncia Artificial (IA) que utiliza redes neuronais complexas para aprender e melhorar automaticamente a partir de grandes conjuntos de dados. Essa t√©cnica permite que os sistemas computacionais aprendam a reconhecer padr√µes e fazer predi√ß√µes baseadas em grandes volumes de dados sem a necessidade de programas explicitos. A Deep Learning √© uma das √°reas mais ativas da IA, com aplica√ß√µes em imagens, √°udio, texto, v√≠deo e muitas outras √°reas.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "# Exemplo simples de chat\n",
    "response = ollama.chat(\n",
    "    model=\"llama3.2:3b\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Explique o que √© Deep Learning em 3 frases.\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Chat com contexto (m√∫ltiplas mensagens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN (Redes Neurais Recorrentes) e Transformer s√£o duas abordagens comuns para processamento de linguagem natural (NLP). Embora ambas sejam usadas em aplicativos de NLP, elas diferem significativamente na forma como lidam com a sequ√™ncia temporal de texto.\n",
      "\n",
      "**Rede Neural Recorrente (RNN):**\n",
      "\n",
      "Uma RNN processa a sequ√™ncia de texto de uma √∫nica c√©lula no tempo. A sa√≠da da c√©lula √© passada para a pr√≥xima c√©lula, criando um efeito \"recorrente\" que permite ao modelo capturar padr√µes em longas sequ√™ncias.\n",
      "\n",
      "**Vantagens:**\n",
      "\n",
      "*   F√°cil implementa√ß√£o\n",
      "*   Capaz de lidar com s√©ries temporais curtas\n",
      "\n",
      "**Desvantagens:**\n",
      "\n",
      "*   Comporta-se menos bem com s√©ries temporais longas\n",
      "*   Suficiente para um modelo que usa uma camada de recurso √∫nica.\n",
      "\n",
      "**Transformador:**\n",
      "\n",
      "Uma rede neural transformadora √© uma rede que processa a sequ√™ncia de texto como uma s√©rie de blocos independentes, cada bloco sendo transformado usando o aten√ß√£o de \"self\" e \"cross\". Isso permite ao modelo capturar padr√µes em longas sequ√™ncias sem sofrer com a limita√ß√£o da sequ√™ncia temporal.\n",
      "\n",
      "**Vantagens:**\n",
      "\n",
      "*   Comporta-se bem com s√©ries temporais longas\n",
      "*   Maior efici√™ncia computacional\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Voc√™ √© um assistente especializado em IA. Seja conciso e t√©cnico.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Qual a diferen√ßa entre RNN e Transformer?\"},\n",
    "]\n",
    "\n",
    "response = ollama.chat(model=\"llama3.2:3b\", messages=messages)\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Streaming (resposta em tempo real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Llama 3.2: Claro, aqui est√£o tr√™s aplica√ß√µes pr√°ticas de LLMs (Modelos de Linguagem de Grande Escala):\n",
      "\n",
      "1. **Assistentes Virtuais**: Os LLMs podem ser usados para criar assistentes virtuais que possam entender e responder a perguntas de forma mais inteligente. Por exemplo, o modelo de linguagem do Google Bard pode ser usado para criar um assistente virtual que possa responder a perguntas de forma mais precisa e eficiente.\n",
      "\n",
      "2. **Genera√ß√£o de Conte√∫do**: Os LLMs podem ser usados para gerar conte√∫do autom√°tico, como textos, imagens e v√≠deos. Por exemplo, o modelo de linguagem da Microsoft Azure Canva pode ser usado para criar uma ferramenta de design gr√°fico que possa gerar imagens personalizadas.\n",
      "\n",
      "3. **An√°lise de Texto**: Os LLMs podem ser usados para analisar texto de forma mais precisa e eficiente. Por exemplo, o modelo de linguagem do IBM Watson pode ser usado para analisar sentimentos em textos, como avalia√ß√µes de produtos ou coment√°rios de clientes.\n",
      "\n",
      "Essas s√£o apenas algumas das muitas aplica√ß√µes pr√°ticas que os LLMs podem ser usados em. Al√©m disso, a tecnologia est√° evoluindo rapidamente e novas aplica√ß√µes est√£o sendo descobertas todos os dias!\n"
     ]
    }
   ],
   "source": [
    "print(\"ü§ñ Llama 3.2: \", end=\"\")\n",
    "for chunk in ollama.chat(\n",
    "    model=\"llama3.2:3b\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Liste 3 aplica√ß√µes pr√°ticas de LLMs.\"}],\n",
    "    stream=True\n",
    "):\n",
    "    print(chunk[\"message\"][\"content\"], end=\"\", flush=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Gera√ß√£o direta (sem chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A resposta correta seria:\n",
      "\n",
      "A intelig√™ncia artificial √© uma t√©cnica e √°rea da inform√°tica que visa criar m√°quinas capazes de simular os processos do pensamento humano, permitindo que elas tomem decis√µes, aprendam com dados e se adaptem a novas situa√ß√µes.\n"
     ]
    }
   ],
   "source": [
    "# Modo generate (mais simples para tarefas √∫nicas)\n",
    "response = ollama.generate(\n",
    "    model=\"llama3.2:3b\",\n",
    "    prompt=\"Complete: A intelig√™ncia artificial √©\"\n",
    ")\n",
    "print(response[\"response\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Par√¢metros avan√ßados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claro! Aqui est√£o algumas sugest√µes de t√≠tulos criativos para um artigo sobre IA:\n",
      "\n",
      "1. **\"A Intelig√™ncia Artificial que nos Mudar√° a Vida\"**\n",
      "2. **\"O Futuro da Intel\n"
     ]
    }
   ],
   "source": [
    "response = ollama.chat(\n",
    "    model=\"llama3.2:3b\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Gerar um t√≠tulo criativo para um artigo sobre IA.\"}],\n",
    "    options={\n",
    "        \"temperature\": 0.9,\n",
    "        \"num_predict\": 50,\n",
    "        \"top_p\": 0.9,\n",
    "        \"top_k\": 40,\n",
    "    }\n",
    ")\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Via API REST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ol√°! Como posso ajudar voc√™ hoje?\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "url = \"http://localhost:11434/api/chat\"\n",
    "payload = {\n",
    "    \"model\": \"llama3.2:3b\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Diga ol√° em portugu√™s.\"}],\n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=payload)\n",
    "result = response.json()\n",
    "print(result[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Informa√ß√µes do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo: llama3.2:3b\n",
      "Fam√≠lia: llama\n",
      "Par√¢metros: 3.2B\n",
      "Quantiza√ß√£o: Q4_K_M\n"
     ]
    }
   ],
   "source": [
    "info = ollama.show(\"llama3.2:3b\")\n",
    "print(f\"Modelo: llama3.2:3b\")\n",
    "print(f\"Fam√≠lia: {info.get('details', {}).get('family', 'N/A')}\")\n",
    "print(f\"Par√¢metros: {info.get('details', {}).get('parameter_size', 'N/A')}\")\n",
    "print(f\"Quantiza√ß√£o: {info.get('details', {}).get('quantization_level', 'N/A')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcdia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
