{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62a0a3d3",
   "metadata": {},
   "source": [
    "# Analisar discursos\n",
    "\n",
    "O objetivo desse notebook é analisar discursos disponíveis no diretório _data. Para preparar uma nova base, execute o notebook 01_preparar_base_discursos.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd08135",
   "metadata": {},
   "source": [
    "## Carregar dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79af89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a06086",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "#from pathlib import Path\n",
    "\n",
    "#DATA = Path(\"_data/discursos_2019-02-01_2023-01-31.parquet\")\n",
    "#df = pd.read_parquet(DATA)\n",
    "\n",
    "data_files = {\"train\": \"data/full/discursos_2019-02-01_2023-01-31.parquet\"}\n",
    "dataset = load_dataset(\"fabriciosantana/discursos-senado-legislatura-56\", data_files=data_files)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b4eed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dataset[\"train\"])\n",
    "#df = dataset[\"train\"].to_pandas()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4263157c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Data\"] = pd.to_datetime(df[\"Data\"], errors=\"coerce\")\n",
    "df = df[df[\"ok\"].eq(True) & df[\"TextoDiscursoIntegral\"].notna() & df[\"TextoDiscursoIntegral\"].str.len().gt(0)]\n",
    "df[\"ano\"] = df[\"Data\"].dt.year\n",
    "df[\"mes\"] = df[\"Data\"].dt.to_period(\"M\").astype(str)  # ex.: '2019-03'\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d50974",
   "metadata": {},
   "source": [
    "## Mapa de temas (tópicos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1bca38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from unidecode import unidecode\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def normalize(s):\n",
    "    s = unidecode((s or \"\").lower())\n",
    "    s = re.sub(r\"[^a-z0-9áéíóúãõç\\s]\", \" \", s)  # simples; ajuste se quiser\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "df[\"texto_clean\"] = df[\"TextoDiscursoIntegral\"].apply(normalize)\n",
    "\n",
    "# embeddings (semântica)\n",
    "model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "emb = model.encode(df[\"texto_clean\"].tolist(), batch_size=64, show_progress_bar=True, normalize_embeddings=True)\n",
    "\n",
    "# clusterização\n",
    "k = 20  # ajuste depois de olhar coerência\n",
    "km = KMeans(n_clusters=k, n_init=10, random_state=42)\n",
    "df[\"topico\"] = km.fit_predict(emb)\n",
    "\n",
    "# palavras-chave por tópico via TF-IDF médio (c-TF-IDF simplificado)\n",
    "vec = TfidfVectorizer(ngram_range=(1,2), min_df=5, max_df=0.8)\n",
    "X = vec.fit_transform(df[\"texto_clean\"])\n",
    "vocab = np.array(vec.get_feature_names_out())\n",
    "\n",
    "def top_terms_for_topic(t, n=12):\n",
    "    idx = np.where(df[\"topico\"].values == t)[0]\n",
    "    if len(idx)==0: return []\n",
    "    mean_tfidf = X[idx].mean(axis=0).A1\n",
    "    return vocab[mean_tfidf.argsort()[::-1][:n]].tolist()\n",
    "\n",
    "termos = {t: top_terms_for_topic(t) for t in sorted(df[\"topico\"].unique())}\n",
    "pd.DataFrame({\"topico\": list(termos.keys()), \"termos\": list(termos.values())}).head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a3a64b",
   "metadata": {},
   "source": [
    "### Tendências (mês × tópico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afc9f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trend = (df.groupby([\"mes\",\"topico\"])\n",
    "           .size()\n",
    "           .reset_index(name=\"qtd\"))\n",
    "# se quiser por partido: .groupby([\"mes\",\"topico\",\"Partido\"]).size()...\n",
    "trend.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4d1390",
   "metadata": {},
   "source": [
    "### Tom/sentimento (baseline) e variações por grupo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f23ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = {\"importante\",\"saude\",\"melhoria\",\"progresso\",\"direito\",\"justica\",\"educacao\"}\n",
    "neg = {\"crise\",\"problema\",\"corrupcao\",\"retrocesso\",\"corte\",\"injustica\",\"deficit\"}\n",
    "\n",
    "def lex_sent(s):\n",
    "    toks = normalize(s).split()\n",
    "    p = sum(w in pos for w in toks); n = sum(w in neg for w in toks)\n",
    "    return (p - n) / max(1, len(toks))\n",
    "\n",
    "df[\"sent_lex\"] = df[\"TextoDiscursoIntegral\"].apply(lex_sent)\n",
    "sent_partido = df.groupby(\"Partido\")[\"sent_lex\"].mean().sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c29cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_oradores = (df.groupby([\"topico\",\"NomeAutor\"])\n",
    "                  .size()\n",
    "                  .reset_index(name=\"falas\")\n",
    "                  .sort_values([\"topico\",\"falas\"], ascending=[True,False]))\n",
    "top_partidos = (df.groupby([\"topico\",\"Partido\"])\n",
    "                  .size()\n",
    "                  .reset_index(name=\"falas\")\n",
    "                  .sort_values([\"topico\",\"falas\"], ascending=[True,False]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b752d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_lg\")  # python -m spacy download pt_core_news_lg\n",
    "\n",
    "def ents(text):\n",
    "    doc = nlp(text[:20000])  # limite por desempenho\n",
    "    return [(e.text, e.label_) for e in doc.ents]\n",
    "\n",
    "sample = df.sample(500, random_state=42).copy()  # rode em lote\n",
    "sample[\"ents\"] = sample[\"TextoDiscursoIntegral\"].apply(ents)\n",
    "\n",
    "# co-ocorrência simples (pessoas dentro do mesmo discurso)\n",
    "from collections import Counter\n",
    "pairs = Counter()\n",
    "for lst in sample[\"ents\"]:\n",
    "    ps = sorted({e for e,l in lst if l in (\"PER\",\"ORG\")})\n",
    "    for i in range(len(ps)):\n",
    "        for j in range(i+1, len(ps)):\n",
    "            pairs[(ps[i], ps[j])] += 1\n",
    "\n",
    "pd.DataFrame([{\"a\":a,\"b\":b,\"peso\":w} for (a,b),w in pairs.items()]).sort_values(\"peso\", ascending=False).head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70486c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"_data/discursos_2019-01-01_2023-01-31.csv\", sep=\";\")\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
