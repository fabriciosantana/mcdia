{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fabriciosantana/mcdia/blob/main/01-icd/assignments/01-preparar-base-discursos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4cb7fa5",
      "metadata": {
        "id": "b4cb7fa5"
      },
      "source": [
        "# Preparar Base de Dados com Discursos do Senado\n",
        "\n",
        "Ao final deste notebook é esperado que se tenha uma base de dados (arquivo parquet) no diretório _data. Para evitar fazer download de novos dados, é dado a opção de reaproveitar arquivo baixando anteriormente para o mesmo período de data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79f87330",
      "metadata": {
        "id": "79f87330"
      },
      "source": [
        "## Instalar dependências\n",
        "\n",
        "Instale as dependências necessárias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "837c0081",
      "metadata": {
        "id": "837c0081"
      },
      "outputs": [],
      "source": [
        "\n",
        "%pip install requests pandas pyarrow\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21a0c748",
      "metadata": {
        "id": "21a0c748"
      },
      "source": [
        "## Importar bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff3ddabb",
      "metadata": {
        "id": "ff3ddabb"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import datetime as dt\n",
        "import logging\n",
        "import requests\n",
        "import pandas as pd\n",
        "import re\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any\n",
        "from requests.adapters import HTTPAdapter, Retry\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from pprint import pformat"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1cc22a7",
      "metadata": {
        "id": "f1cc22a7"
      },
      "source": [
        "## Inicializar variáveis globais"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f43f4a7d",
      "metadata": {
        "id": "f43f4a7d"
      },
      "outputs": [],
      "source": [
        "BASE = \"https://legis.senado.leg.br/dadosabertos/\"\n",
        "\n",
        "TIMEOUT_JSON = 90\n",
        "TIMEOUT_TXT  = 60\n",
        "RETRY_TOTAL  = 8\n",
        "RETRY_BACKOFF = 0.6\n",
        "STATUS_FORCELIST = [429, 500, 502, 503, 504]\n",
        "\n",
        "DEFAULT_INI = dt.date(2019, 3, 29)\n",
        "DEFAULT_FIM = dt.date(2019, 3, 31)\n",
        "\n",
        "DATA_DIR = Path(\"_data\")\n",
        "DATA_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s [%(levelname)s] %(message)s\"\n",
        ")\n",
        "log = logging.getLogger(\"discursos\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f50bbaf8",
      "metadata": {
        "id": "f50bbaf8"
      },
      "source": [
        "## Definir métodos utilitários\n",
        "\n",
        "Execute as céluas abaixo para definir métodos que serão utilizados na preparação da base de dados"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f4f6590",
      "metadata": {
        "id": "5f4f6590"
      },
      "source": [
        "### Criar sessão HTTP\n",
        "\n",
        "Os dados são obtidos do portal de dados abertos do Senado por meio de requisições HTTP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b801f19",
      "metadata": {
        "id": "7b801f19"
      },
      "outputs": [],
      "source": [
        "def make_session() -> requests.Session:\n",
        "    s = requests.Session()\n",
        "    retries = Retry(\n",
        "        total=RETRY_TOTAL,\n",
        "        backoff_factor=RETRY_BACKOFF,\n",
        "        status_forcelist=STATUS_FORCELIST,\n",
        "        allowed_methods=[\"GET\"],\n",
        "    )\n",
        "    s.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
        "\n",
        "    log.info(f\">>> Headers: {pformat(dict(s.headers))}\")\n",
        "    log.info(f\">>> Cookies: {s.cookies}\")\n",
        "    log.info(f\">>> Auth: {s.auth}\")\n",
        "\n",
        "    return s\n",
        "\n",
        "log.info(f\"Construindo sessão HTTP\")\n",
        "sess = make_session()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25c7d6c3",
      "metadata": {
        "id": "25c7d6c3"
      },
      "source": [
        "### Montar intervalo de datas\n",
        "\n",
        "As requisições à API do Senado só pode ser realizada para intervalos de 31 dias. A partir de um intervalo de datas, o método abaixo criar intervalos no período pré-definido."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b6983f6",
      "metadata": {
        "id": "6b6983f6"
      },
      "outputs": [],
      "source": [
        "def montar_intervalo_de_datas(start: dt.date, end: dt.date, days_per_window: int = 31) -> List[tuple]:\n",
        "    \"\"\"Gera janelas [ini, fim] inclusive, com no máx. 'days_per_window' dias cada.\"\"\"\n",
        "\n",
        "    log.info(f\">>> Montando intervalos de {days_per_window} dias para fazer download dos discursos em blocos\")\n",
        "\n",
        "    windows = []\n",
        "    cur = start\n",
        "    one_day = dt.timedelta(days=1)\n",
        "\n",
        "    while cur <= end:\n",
        "        w_end = min(cur + dt.timedelta(days=days_per_window - 1), end)\n",
        "        windows.append((cur, w_end))\n",
        "        log.info(f\">>> >>> Janela {len(windows)}: {(cur, w_end)}\")\n",
        "        cur = w_end + one_day\n",
        "\n",
        "    log.info(f\">>> O download dos discursos será realizado em {len(windows)} intervalos\")\n",
        "    return windows"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe97875d",
      "metadata": {
        "id": "fe97875d"
      },
      "source": [
        "### Extrair discurso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "420181b3",
      "metadata": {
        "id": "420181b3"
      },
      "outputs": [],
      "source": [
        "def extrair_discurso(obj: Any) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Procura, recursivamente, qualquer lista sob a chave 'Pronunciamento'.\n",
        "    Isso torna o parser resiliente a pequenas mudanças de envelope.\n",
        "    \"\"\"\n",
        "    out = []\n",
        "\n",
        "    def rec(x):\n",
        "        if isinstance(x, dict):\n",
        "            for k, v in x.items():\n",
        "                if isinstance(k, str) and k.lower() == \"pronunciamento\" and isinstance(v, list):\n",
        "                    out.extend(v)\n",
        "                else:\n",
        "                    rec(v)\n",
        "        elif isinstance(x, list):\n",
        "            for it in x:\n",
        "                rec(it)\n",
        "\n",
        "    rec(obj)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02d6bd8e",
      "metadata": {
        "id": "02d6bd8e"
      },
      "source": [
        "### Recuperar discurso de um período\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7815477",
      "metadata": {
        "id": "d7815477"
      },
      "outputs": [],
      "source": [
        "def recuperar_lista_discursos_por_periodo(data_inicio: dt.date, data_fim: dt.date, sleep_s: float = 0.0) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Busca discursos do Plenário por janelas de data, agregando tudo num DataFrame.\n",
        "    Endpoint: /plenario/lista/discursos/{AAAAMMDD}/{AAAAMMDD}.json\n",
        "    \"\"\"\n",
        "\n",
        "    log.info(f\">>> Preparando intervalos para download dos discursos de {data_inicio} a {data_fim}\")\n",
        "    windows = montar_intervalo_de_datas(data_inicio, data_fim, days_per_window=31)  # janelas de ~1 mês\n",
        "\n",
        "    log.info(f\">>> Iniciando download dos discursos em {len(windows)} intervalos\")\n",
        "    all_rows = []\n",
        "\n",
        "    for i, (data_ini, data_fim) in enumerate(windows, 1):\n",
        "        url = f\"{BASE}plenario/lista/discursos/{data_ini.strftime('%Y%m%d')}/{data_fim.strftime('%Y%m%d')}.json\"\n",
        "\n",
        "        log.info(f\">>> >>> GET: {url}\")\n",
        "        r = sess.get(url, headers={\"Accept\": \"application/json\"}, timeout=TIMEOUT_JSON)\n",
        "        r.raise_for_status()\n",
        "        j = r.json()\n",
        "\n",
        "        log.info(f\">>> >>> Extraindo discurso\")\n",
        "        pron = extrair_discurso(j)\n",
        "        if pron:\n",
        "            # flatten resiliente\n",
        "            df = pd.json_normalize(pron, sep=\".\")\n",
        "            # anexa janela de coleta (útil para auditoria)\n",
        "            df[\"__janela_inicio\"] = data_ini.isoformat()\n",
        "            df[\"__janela_fim\"] = data_fim.isoformat()\n",
        "            all_rows.append(df)\n",
        "            log.info(f\">>> >>> Discursos extraídos: {len(all_rows)}\")\n",
        "\n",
        "        if sleep_s:\n",
        "            time.sleep(sleep_s)\n",
        "\n",
        "    if not all_rows:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    df_all = pd.concat(all_rows, ignore_index=True, sort=False)\n",
        "    log.info(f\">>> Discursos recuperados: {len(df_all)}\")\n",
        "\n",
        "    return df_all"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12b781de",
      "metadata": {
        "id": "12b781de"
      },
      "source": [
        "### Recuperar texto integral de um discurso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f56ffc05",
      "metadata": {
        "id": "f56ffc05"
      },
      "outputs": [],
      "source": [
        "def recuperar_texto_discurso(codigo_pron: str, url_txt: str) -> dict:\n",
        "    out = {\"CodigoPronunciamento\": codigo_pron, \"TextoDiscursoIntegral\": \"\", \"ok\": False, \"status\": None, \"msg\": \"\"}\n",
        "\n",
        "    try:\n",
        "        log.info(f\">>> GET: {url_txt}\")\n",
        "        r = sess.get(url_txt, timeout=TIMEOUT_TXT, headers={\"Accept\": \"text/plain, */*;q=0.1\"}, allow_redirects=True)\n",
        "        out[\"status\"] = r.status_code\n",
        "\n",
        "        # log auxiliar para diagnosticar\n",
        "        ct = (r.headers.get(\"Content-Type\") or \"\").lower()\n",
        "        if r.status_code == 404:\n",
        "            out[\"msg\"] = \"404 (sem texto integral)\";  return out\n",
        "        if r.status_code == 204:\n",
        "            out[\"msg\"] = \"204 (sem conteúdo)\";        return out\n",
        "\n",
        "        r.raise_for_status()\n",
        "\n",
        "        # conteúdo\n",
        "        txt = r.text or \"\"\n",
        "        # limpeza leve (uso de re.sub, não r.sub)\n",
        "        txt = re.sub(r\"\\s+\\n\", \"\\n\", txt)\n",
        "        txt = re.sub(r\"[ \\t]+\", \" \", txt).strip()\n",
        "\n",
        "        # proteção: se veio vazio, registra e sai\n",
        "        if not txt:\n",
        "            out[\"msg\"] = f\"vazio (Content-Type={ct})\"\n",
        "            return out\n",
        "\n",
        "        out[\"ok\"] = True\n",
        "        out[\"TextoDiscursoIntegral\"] = txt\n",
        "        return out\n",
        "\n",
        "    except Exception as e:\n",
        "        out[\"msg\"] = str(e)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "221c93e0",
      "metadata": {
        "id": "221c93e0"
      },
      "source": [
        "### Preparar discurso para download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3aeadd5",
      "metadata": {
        "id": "f3aeadd5"
      },
      "outputs": [],
      "source": [
        "def preparar_discursos_para_download(\n",
        "    df_discursos: pd.DataFrame,\n",
        "    cols_necessarias=(\"TextoIntegralTxt\", \"CodigoPronunciamento\"),\n",
        "    inplace: bool = False\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    - Faz rename tolerante das colunas necessárias para os nomes canônicos em `cols_necessarias`.\n",
        "    - Converte para string e strip.\n",
        "    - Retorna APENAS as linhas com URL válida em `TextoIntegralTxt` (http/https).\n",
        "    \"\"\"\n",
        "    df = df_discursos if inplace else df_discursos.copy()\n",
        "\n",
        "    # 1) garantir/renomear colunas\n",
        "    ren = {}\n",
        "    for alvo in cols_necessarias:\n",
        "        col_real = _match_col(df, alvo)\n",
        "        if col_real != alvo:\n",
        "            ren[col_real] = alvo\n",
        "    if ren:\n",
        "        df = df.rename(columns=ren)\n",
        "\n",
        "    # 2) normalizar valores\n",
        "    for alvo in cols_necessarias:\n",
        "        df[alvo] = df[alvo].astype(str).str.strip()\n",
        "\n",
        "    # 3) filtrar URLs válidas\n",
        "    df_filtrado = df[df[\"TextoIntegralTxt\"].str.startswith((\"http://\", \"https://\"), na=False)].copy()\n",
        "\n",
        "    return df_filtrado\n",
        "\n",
        "def _match_col(df: pd.DataFrame, alvo: str) -> str:\n",
        "    \"\"\"\n",
        "    Tenta encontrar em df.columns a coluna equivalente a `alvo`.\n",
        "    Estratégia: match exato (case-insensitive) -> contém (case-insensitive).\n",
        "    Retorna o nome da coluna encontrada ou levanta KeyError.\n",
        "    \"\"\"\n",
        "    cols = list(df.columns)\n",
        "    # 1) exato (case-insensitive)\n",
        "    for c in cols:\n",
        "        if c.lower() == alvo.lower():\n",
        "            return c\n",
        "    # 2) contém (case-insensitive)\n",
        "    candidatos = [c for c in cols if alvo.lower() in c.lower()]\n",
        "    if candidatos:\n",
        "        # prioriza o mais curto (geralmente o nome mais \"limpo\")\n",
        "        return sorted(candidatos, key=len)[0]\n",
        "    raise KeyError(f\"Coluna obrigatória não encontrada: {alvo}. Disponíveis: {cols}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c07176d",
      "metadata": {
        "id": "8c07176d"
      },
      "source": [
        "### Fazer download texto discurso\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66789a5a",
      "metadata": {
        "id": "66789a5a"
      },
      "outputs": [],
      "source": [
        "def fazer_download_texto_discursos(\n",
        "    df_download,\n",
        "    fetch_fn,                 # ex.: fetch_and_save_txt(codigo_pron, url_txt)\n",
        "    max_workers: int = 8\n",
        "):\n",
        "    \"\"\"\n",
        "    Executa fetch_fn(CodigoPronunciamento, TextoIntegralTxt) em paralelo\n",
        "    para cada linha de df_download e retorna a lista 'resultados'.\n",
        "\n",
        "    - fetch_fn deve retornar um dict (ex.: {\"CodigoPronunciamento\":..., \"ok\":..., ...})\n",
        "    \"\"\"\n",
        "    resultados = []\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
        "        futs = {\n",
        "            ex.submit(fetch_fn, row[\"CodigoPronunciamento\"], row[\"TextoIntegralTxt\"]): row\n",
        "            for _, row in df_download.iterrows()\n",
        "        }\n",
        "        for fut in as_completed(futs):\n",
        "            row = futs[fut]\n",
        "            try:\n",
        "                resultados.append(fut.result())\n",
        "            except Exception as e:\n",
        "                resultados.append({\n",
        "                    \"CodigoPronunciamento\": row.get(\"CodigoPronunciamento\"),\n",
        "                    \"ok\": False,\n",
        "                    \"msg\": str(e),\n",
        "                })\n",
        "    return pd.DataFrame(resultados)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae4c9cd0",
      "metadata": {
        "id": "ae4c9cd0"
      },
      "source": [
        "### Solicitar intervalo de datas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "735bf05a",
      "metadata": {
        "id": "735bf05a"
      },
      "outputs": [],
      "source": [
        "def ler_intervalo_datas() -> tuple[dt.date, dt.date]:\n",
        "    print(\"Informe o intervalo (ENTER para usar padrão 2019-03-29 → 2019-03-31).\")\n",
        "    s_ini = input(\"Data inicial [2019-03-29]: \").strip()\n",
        "    s_fim = input(\"Data final   [2019-03-31]: \").strip()\n",
        "\n",
        "    ini = _parse_data(s_ini) if s_ini else DEFAULT_INI\n",
        "    fim = _parse_data(s_fim) if s_fim else DEFAULT_FIM\n",
        "    if fim < ini:\n",
        "        ini, fim = fim, ini\n",
        "        print(f\"Aviso: datas invertidas. Usando {ini} → {fim}.\")\n",
        "    return ini, fim\n",
        "\n",
        "def _parse_data(s: str) -> dt.date:\n",
        "    s = (s or \"\").strip()\n",
        "    for fmt in (\"%Y-%m-%d\", \"%d/%m/%Y\"):\n",
        "        try:\n",
        "            return dt.datetime.strptime(s, fmt).date()\n",
        "        except ValueError:\n",
        "            pass\n",
        "    raise ValueError(f\"Data inválida: {s!r}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "261bbd46",
      "metadata": {
        "id": "261bbd46"
      },
      "source": [
        "### Perguntar de deseja utilizar arquivo baixado anteriormente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0df8fdda",
      "metadata": {
        "id": "0df8fdda"
      },
      "outputs": [],
      "source": [
        "def ask_yes_no(msg: str, default_yes=True) -> bool:\n",
        "    prompt = \" [S/n] \" if default_yes else \" [s/N] \"\n",
        "    while True:\n",
        "        ans = input(msg + prompt).strip().lower()\n",
        "        if not ans:\n",
        "            return default_yes\n",
        "        if ans in (\"s\",\"sim\",\"y\",\"yes\"): return True\n",
        "        if ans in (\"n\",\"nao\",\"não\",\"no\"): return False\n",
        "        print(\"Responda com s/n.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74bdb700",
      "metadata": {
        "id": "74bdb700"
      },
      "source": [
        "## Fluxo principal para preparação da base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c36c983",
      "metadata": {
        "id": "2c36c983"
      },
      "outputs": [],
      "source": [
        "ini, fim = ler_intervalo_datas()\n",
        "\n",
        "out_path = DATA_DIR / f\"discursos_{ini.isoformat()}_{fim.isoformat()}.parquet\"\n",
        "#out_path = DATA_DIR / f\"discursos_{ini.isoformat()}_{fim.isoformat()}.csv\"\n",
        "\n",
        "if out_path.exists():\n",
        "    if ask_yes_no(f\"Arquivo já existe: {out_path}\\nUsar o arquivo existente?\"):\n",
        "        log.info(f\"Usando: {out_path}\")\n",
        "        df_final = pd.read_parquet(out_path)\n",
        "        #df_final = pd.read_csv(out_path, sep=\";\", dtype=str)\n",
        "        log.info(f\"Discursos existentes no arquivo: {len(df_final)}\")\n",
        "        log.info(f\"OK: {df_final['ok'].eq(True).sum()} textos baixados, {len(df_final)-df_final['ok'].eq(True).sum()} sem texto. Arquivo salvo em: {out_path}\")\n",
        "        #log.info(df_final[\"TextoDiscursoIntegral\"].str.len())\n",
        "        raise SystemExit(0)\n",
        "    else:\n",
        "        log.info(\"Refazendo download dos discursos…\")\n",
        "\n",
        "log.info(f\"Recuperando lista de discursos realizados no período de {ini} a {fim}\")\n",
        "df_discursos = recuperar_lista_discursos_por_periodo(ini, fim, sleep_s=0.0)\n",
        "log.info(f\"Lista de discursos recuperados: {len(df_discursos)}\")\n",
        "\n",
        "log.info(f\"Prepando discursos para download\")\n",
        "df_download = preparar_discursos_para_download(df_discursos)\n",
        "log.info(f\"Discursos com link para download do texto integral {len(df_download)}\")\n",
        "\n",
        "log.info(f\"Iniciando download do texto integral do discurso com link para texto integral: {len(df_download)}\")\n",
        "df_txt = fazer_download_texto_discursos(df_download, recuperar_texto_discurso, max_workers=8)\n",
        "log.info(f\"Foi realizado o download dos textos de discursos: {len(df_txt)}\")\n",
        "\n",
        "df_final = df_discursos.merge(\n",
        "    df_txt[[\"CodigoPronunciamento\", \"TextoDiscursoIntegral\", \"ok\", \"status\", \"msg\"]],\n",
        "    on=\"CodigoPronunciamento\",\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "log.info(f\"Textos no data frame final: {len(df_final)}\")\n",
        "log.info(f\"Discursos por período: {len(df_discursos):,} linhas\")\n",
        "\n",
        "log.info(f\"Salvando arquivo com a lista dos discursos e os respectivos textos integrais\")\n",
        "df_final.to_parquet(out_path, index=False, engine=\"pyarrow\", compression=\"zstd\")\n",
        "#df_final.to_csv(\n",
        "#    out_path,\n",
        "#    index=False,\n",
        "#    sep=\";\")\n",
        "log.info(f\"OK: {df_final['ok'].eq(True).sum()} textos baixados, {len(df_final)-df_final['ok'].eq(True).sum()} sem texto. Arquivo salvo em: {out_path}\")\n",
        "\n",
        "#log.info(df_final[\"TextoDiscursoIntegral\"].str.len())\n",
        "#log.info(df_final[\"TextoDiscursoIntegral\"].str.split().str.len())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
